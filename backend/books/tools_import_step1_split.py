import os
import sys
from docx import Document
from docx.document import Document as _Document
from docx.oxml.text.paragraph import CT_P
from docx.oxml.table import CT_Tbl
from docx.table import _Cell, Table
from docx.text.paragraph import Paragraph

# è·¯å¾„ä¸ä¸Šä¸‹æ–‡ä¿®å¤
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
if root_dir not in sys.path:
    sys.path.append(root_dir)

from backend.tools.tools_sql_connect import db
from backend.tools.global_context import log_queue_ctx


def emit(msg):
    """æ—¥å¿—æ¨é€"""
    print(msg)
    q = log_queue_ctx.get()
    if q: q.put(f"LOG: {msg}")


class WordParser:
    def _iter_block_items(self, parent):
        if isinstance(parent, _Document):
            parent_elm = parent.element.body
        elif isinstance(parent, _Cell):
            parent_elm = parent._tc
        else:
            raise ValueError("Unsupported parent type")
        for child in parent_elm.iterchildren():
            if isinstance(child, CT_P):
                yield Paragraph(child, parent)
            elif isinstance(child, CT_Tbl):
                yield Table(child, parent)

    def _table_to_markdown(self, table: Table) -> str:
        rows_data = []
        for row in table.rows:
            cell_texts = [cell.text.strip().replace("\n", "<br>") for cell in row.cells]
            rows_data.append(f"| {' | '.join(cell_texts)} |")
        if not rows_data: return ""
        header = rows_data[0]
        separator = "|" + "|".join(["---"] * len(table.rows[0].cells)) + "|"
        body = "\n".join(rows_data[1:])
        return f"\n{header}\n{separator}\n{body}\n"

    def parse_docx(self, file_path: str) -> list:
        if not os.path.exists(file_path): return []
        document = Document(file_path)
        chunks = []
        for block in self._iter_block_items(document):
            if isinstance(block, Paragraph):
                lines = block.text.split('\n')
                for line in lines:
                    if line.strip(): chunks.append(line.strip())
            elif isinstance(block, Table):
                md_table = self._table_to_markdown(block)
                if md_table.strip():
                    chunks.append(f"ã€è¡¨æ ¼æ•°æ®ã€‘\n{md_table}")
        return chunks


def execute_split_task(book_id: int):
    emit(f"ğŸ”ª [åˆ‡åˆ†] å¼€å§‹å¤„ç† BookID={book_id}...")

    book = db.execute_query("SELECT * FROM import_books WHERE book_id=%s", (book_id,), fetch_one=True)
    if not book: return {"status": "error", "msg": "ä¹¦æœ¬ä¸å­˜åœ¨"}

    file_path = book['file_path']
    emit(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")

    parser = WordParser()
    try:
        segments = parser.parse_docx(file_path)
    except Exception as e:
        return {"status": "error", "msg": f"è§£æå¤±è´¥: {e}"}

    if not segments:
        return {"status": "error", "msg": "æ–‡æ¡£å†…å®¹ä¸ºç©º"}

    emit(f"âœ… è§£æå®Œæˆï¼Œå…± {len(segments)} ä¸ªæ®µè½ã€‚æ­£åœ¨å†™å…¥æ•°æ®åº“...")

    try:
        conn = db.get_connection()
        with conn.cursor() as cursor:
            # æ¸…ç†æ—§æ•°æ®
            cursor.execute("DELETE FROM book_segments WHERE book_id=%s", (book_id,))
            cursor.execute("DELETE FROM knowledge_fragments WHERE book_id=%s", (book_id,))

            # æ‰¹é‡å†™å…¥
            sql = "INSERT INTO book_segments (book_id, book_name, content, segment_order, is_processed) VALUES (%s, %s, %s, %s, 0)"
            params = [(book_id, book['book_name'], seg, i + 1) for i, seg in enumerate(segments)]
            cursor.executemany(sql, params)

            # æ›´æ–°çŠ¶æ€
            cursor.execute(
                "UPDATE import_books SET total_segments=%s, processed_segments=0, total_fragments=0, imported_fragments=0 WHERE book_id=%s",
                (len(segments), book_id))
            conn.commit()

        emit(f"ğŸ‰ åˆ‡åˆ†å…¥åº“æˆåŠŸï¼")
        return {"status": "success", "msg": f"åˆ‡åˆ†å®Œæˆï¼Œå…± {len(segments)} æ®µ"}
    except Exception as e:
        return {"status": "error", "msg": f"æ•°æ®åº“é”™è¯¯: {e}"}