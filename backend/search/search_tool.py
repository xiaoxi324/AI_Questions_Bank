import sys
import os

# === è·¯å¾„ä¿®å¤ (æ–°å¢) ===
# ç›®çš„ï¼šç¡®ä¿åœ¨ /backend/search/ ç›®å½•ä¸‹ä¹Ÿèƒ½å¯¼å…¥é¡¹ç›®æ ¹ç›®å½•çš„ config.py å’Œ backend.tools
current_dir = os.path.dirname(os.path.abspath(__file__))
# å‘ä¸Šè·³ä¸¤çº§: search -> backend -> root
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)
# ======================

import json
import chromadb
from typing import List, Dict, Any
from pydantic import BaseModel
from config import config

# [ä¿®æ”¹å¯¼å…¥] æŒ‡å‘æ–°ä½ç½® backend.tools
from backend.tools.tools_call_ai import call_ai_emb
from backend.tools.tools_sql_connect import db


# ==================== æ¨¡å‹å®šä¹‰ ====================
class SearchToolRequest(BaseModel):
    keyword: str


class KnowledgeUpdateRequest(BaseModel):
    id: str
    content: str


# ==================== åŸºç¡€é…ç½® ====================
DB_PATH = getattr(config, "VECTOR_DB_PATH_MEDIC", "G:/KnowledgeBase/vectorizer_medic")
EMBEDDING_DIM = getattr(config, "EMBEDDING_DIM", 4096)


# ==================== è¾…åŠ©å‡½æ•° ====================
def get_search_collections() -> List[str]:
    try:
        sql = "SELECT config_value FROM system_config WHERE config_key = 'search_collections'"
        res = db.execute_query(sql, fetch_one=True)
        if res and res['config_value']:
            return json.loads(res['config_value'])
    except Exception as e:
        print(f"âš ï¸ è¯»å–é…ç½®å¤±è´¥: {e}")

    # [ä¿®æ”¹ç‚¹1] é»˜è®¤å€¼æ”¹ä¸ºæ–°çš„é›†åˆåï¼Œé˜²æ­¢æ•°æ®åº“æ²¡é…ç½®æ—¶å‡ºé”™
    return ["Pharmacopoeia_Official"]


class ChromaManager:
    _client = None

    @classmethod
    def get_client(cls):
        if cls._client is None:
            if not os.path.exists(DB_PATH): return None
            try:
                cls._client = chromadb.PersistentClient(path=DB_PATH)
            except:
                return None
        return cls._client

    @classmethod
    def get_collection(cls, name: str):
        client = cls.get_client()
        return client.get_collection(name=name) if client else None


def _core_search(query_text: str, top_k: int = 10) -> List[Dict]:
    """åº•å±‚é€šç”¨æ£€ç´¢"""
    target_cols = get_search_collections()
    if not target_cols: return []

    query_emb = call_ai_emb(query_text, dimensions=EMBEDDING_DIM)
    if not query_emb: return []

    all_candidates = []
    for col_name in target_cols:
        col = ChromaManager.get_collection(col_name)
        if not col: continue
        try:
            results = col.query(
                query_embeddings=[query_emb],
                n_results=top_k,
                include=["metadatas", "documents", "distances"]
            )
            if not results['metadatas'] or not results['metadatas'][0]: continue

            for i in range(len(results['metadatas'][0])):
                score = 1 - results['distances'][0][i]
                all_candidates.append({
                    "id": results['ids'][0][i],
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "raw_score": score,
                    "source_collection": col_name,
                    # ä¿ç•™ vector_text ç”¨äºåç»­é‡æ’
                    "vector_text": results['documents'][0][i]
                })
        except:
            continue

    all_candidates.sort(key=lambda x: x['raw_score'], reverse=True)
    return all_candidates[:top_k]


# ==================== ä¸šåŠ¡é€»è¾‘ (å·²é€šç”¨åŒ–) ====================

def search_knowledge_structured(query_main: str, query_sub: str = None) -> List[Dict[str, Any]]:
    """
    ã€é€šç”¨çŸ¥è¯†åº“æ£€ç´¢ã€‘
    """
    full_query = f"{query_main} {query_sub}" if query_sub else query_main
    print(f"ğŸ” [RAG] é€šç”¨æ£€ç´¢: {full_query}")

    raw_results = _core_search(query_text=full_query, top_k=20)

    structured_output = []
    for item in raw_results:
        meta = item['metadata']
        content = item['content']
        score = item['raw_score']

        # === åŠ¨æ€æƒé‡ä¼˜åŒ– ===
        boost = 0.0
        if query_main and query_main in content: boost += 0.2
        if query_sub and query_sub in content: boost += 0.1
        final_score = score + boost

        # === L1-L8 è·¯å¾„æ„å»º (æ— éœ€ä¿®æ”¹ï¼Œè¿™éƒ¨åˆ†é€»è¾‘æ˜¯é€šç”¨çš„) ===
        hierarchy_parts = []
        last_valid_node = "æœªå‘½åèŠ‚ç‚¹"

        for i in range(1, 9):
            val = meta.get(f"L{i}")
            if val and str(val).strip():
                hierarchy_parts.append(str(val).strip())
                last_valid_node = str(val).strip()

        path_str = "/".join(hierarchy_parts)

        # æ ‡é¢˜ç­–ç•¥
        title = meta.get("ç»„åˆæ ‡é¢˜")
        if not title:
            title = last_valid_node

        structured_output.append({
            "id": item['id'],
            "source": f"{meta.get('æ¥æºæ–‡ä»¶', 'Base')} | {title}",
            "path": path_str,
            "content": content,
            "raw_score": final_score,
            "score": f"{min(final_score, 0.99):.2%}",
            "_meta_hierarchy": hierarchy_parts
        })

    structured_output.sort(key=lambda x: x['raw_score'], reverse=True)
    return structured_output


def handle_tool_search(req: SearchToolRequest):
    try:
        parts = req.keyword.strip().split(maxsplit=1)
        if not parts: return {"status": "success", "data": []}

        q_main = parts[0]
        q_sub = parts[1] if len(parts) > 1 else None

        results = search_knowledge_structured(query_main=q_main, query_sub=q_sub)
        return {"status": "success", "data": results}
    except Exception as e:
        return {"status": "error", "msg": str(e)}


def update_knowledge_record(doc_id: str, new_content: str) -> bool:
    """
    æ›´æ–°è®°å½•ï¼šç¡®ä¿æ ¼å¼ä¸å…¥åº“æ—¶ä¿æŒä¸€è‡´ (æ ‡é¢˜ + å†…å®¹)
    """
    target_cols = get_search_collections()
    if not target_cols: return False

    updated_any = False
    for col_name in target_cols:
        col = ChromaManager.get_collection(col_name)
        if not col: continue
        try:
            # 1. å…ˆæŸ¥å‡ºåŸå§‹å…ƒæ•°æ®ï¼ˆä¸ºäº†è·å–æ ‡é¢˜ï¼‰
            existing = col.get(ids=[doc_id], include=["metadatas"])
            if not existing or not existing['ids']:
                continue

            # [ä¿®æ”¹ç‚¹2] ä¿æŒå‘é‡æ–‡æœ¬æ ¼å¼ä¸€è‡´æ€§
            current_meta = existing['metadatas'][0]
            combo_title = current_meta.get("ç»„åˆæ ‡é¢˜", "")

            # é‡æ–°æ‹¼æ¥å‘é‡æ–‡æœ¬
            if combo_title:
                new_vector_text = f"{combo_title}ï¼š\n{new_content}"
            else:
                new_vector_text = new_content

            # å‘é‡åŒ–
            new_emb = call_ai_emb(new_vector_text, dimensions=EMBEDDING_DIM)
            if not new_emb: return False

            print(f"ğŸ”„ æ›´æ–°é›†åˆ [{col_name}] ID={doc_id}")

            # æ›´æ–° Documents, Embeddings, é¡ºä¾¿æ›´æ–°å­—æ•°ç»Ÿè®¡
            current_meta["å­—æ•°"] = len(new_content)
            current_meta["ç‰‡æ®µå†…å®¹"] = new_content  # ä¹Ÿå¯ä»¥é€‰æ‹©åŒæ­¥æ›´æ–°å…ƒæ•°æ®é‡Œçš„å†…å®¹å‰¯æœ¬

            col.update(
                ids=[doc_id],
                documents=[new_vector_text],  # è¿™é‡Œå­˜å…¥çš„æ˜¯æ‹¼æ¥åçš„æ–‡æœ¬
                embeddings=[new_emb],
                metadatas=[current_meta]
            )
            updated_any = True
            break
        except Exception as e:
            print(f"âŒ æ›´æ–°å¤±è´¥: {e}")
            continue

    return updated_any


def handle_tool_update(req: KnowledgeUpdateRequest):
    try:
        success = update_knowledge_record(req.id, req.content)
        return {"status": "success", "msg": "æ›´æ–°æˆåŠŸ"} if success else {"status": "error", "msg": "å¤±è´¥"}
    except Exception as e:
        return {"status": "error", "msg": str(e)}


def detailed_read_only_test(test_keyword: str = "ç¦å¿Œ"):
    """
    ã€åªè¯»ã€‘è¯¦ç»†æ£€ç´¢æµ‹è¯•
    """
    print("\n" + "=" * 60)
    print(f"ğŸ§ª å¼€å§‹åªè¯»æµ‹è¯• | å…³é”®è¯: [{test_keyword}]")
    print("=" * 60)

    # 1. æ£€æŸ¥é…ç½®
    target_collections = get_search_collections()
    print(f"ğŸ“‹ ç›®æ ‡é›†åˆ: {target_collections}")
    print(f"ğŸ“‚ æ•°æ®åº“è·¯å¾„: {DB_PATH}")

    # 2. æ£€æŸ¥ AI è¿æ¥ (ç¡®ä¿å‘é‡åŒ–æ­£å¸¸)
    print("ğŸ”Œ æ­£åœ¨æ£€æŸ¥ AI å‘é‡åŒ–æœåŠ¡...", end="")
    try:
        test_emb = call_ai_emb("test", dimensions=EMBEDDING_DIM)
        if test_emb and len(test_emb) == EMBEDDING_DIM:
            print(" âœ… è¿æ¥æ­£å¸¸")
        else:
            print(" âŒ å‘é‡åŒ–ç»“æœä¸ºç©ºæˆ–ç»´åº¦é”™è¯¯")
            return
    except Exception as e:
        print(f" âŒ è¿æ¥å¤±è´¥: {e}")
        return

    # 3. æ‰§è¡Œæ£€ç´¢
    print(f"ğŸ” æ­£åœ¨æ‰§è¡Œæ£€ç´¢: '{test_keyword}' ...")
    start_time = __import__('time').time()

    # è°ƒç”¨ä½ çš„ä¸šåŠ¡æ£€ç´¢å‡½æ•°
    results = search_knowledge_structured(query_main=test_keyword)

    cost_time = __import__('time').time() - start_time
    print(f"â±ï¸ è€—æ—¶: {cost_time:.4f}s | æ‰¾åˆ°ç»“æœ: {len(results)} æ¡")

    if not results:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚")
        return

    # 4. è¯¦ç»†å±•ç¤ºå‰ 3 æ¡ç»“æœ
    print("\n" + "-" * 30 + " ç»“æœè¯¦æƒ… " + "-" * 30)

    for i, item in enumerate(results[:3], 1):
        print(f"\nğŸ·ï¸ [ç»“æœ {i}] (åŒ¹é…åº¦: {item['score']})")
        print(f"ğŸ†” ID: {item['id']}")

        # é‡ç‚¹éªŒè¯ L1-L8 è·¯å¾„æ˜¯å¦è§£ææ­£ç¡®
        print(f"ğŸ›¤ï¸ è§£æè·¯å¾„ (L1-L8): {item['path']}")

        # éªŒè¯æ ‡é¢˜
        source_info = item.get('source', '')
        print(f"ğŸ“š æ¥æº/æ ‡é¢˜: {source_info}")

        # éªŒè¯å†…å®¹é¢„è§ˆ
        content_preview = item['content'].replace('\n', ' ')[:100]
        print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {content_preview}...")

        # --- è°ƒè¯•ç”¨ï¼šæ‰“å°åŸå§‹å…ƒæ•°æ® ---
        print(f"ğŸ”§ [è°ƒè¯•] Lå±‚çº§åŸå§‹å€¼: {item.get('_meta_hierarchy', [])}")

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•ç»“æŸ")
    print("=" * 60)


if __name__ == "__main__":
    detailed_read_only_test("å¤±çœ ")