import os
import json
import chromadb
from chromadb.api.types import Documents, EmbeddingFunction, Embeddings
from config import config
from backend.tools.tools_call_ai import call_ai_emb

# ==============================================================================
# ğŸ› ï¸ ã€é…ç½®åŒºåŸŸã€‘è¯·åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°
# ==============================================================================

# 1. æŒ‡å®šè¦å…¥åº“çš„ JSON æ–‡ä»¶ç»å¯¹è·¯å¾„ (ç²¾ç¡®åˆ° .json æ–‡ä»¶)
TARGET_JSON_PATH = r"G:\KnowledgeBase\åˆ†è¯åæ•°æ®\è¯å…¸ä¸´åºŠç”¨è¯é¡»çŸ¥.json"

# 2. ç›®æ ‡é›†åˆåç§° (æƒ³å­˜åˆ°å“ªä¸ªé›†åˆå°±å¡«å“ªä¸ª)
TARGET_COLLECTION_NAME = "Pharmacopoeia_Official"
# Hospital_Pharmac/Pharmacopoeia_Official/Pharmacopoeia_Proficiency

# 3. å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
VECTOR_DB_PATH = getattr(config, "VECTOR_DB_PATH_MEDIC", "G:/KnowledgeBase/vectorizer_medic")

# 4. æ˜¯å¦å…ˆæ¸…ç©ºè¯¥é›†åˆï¼Ÿ (True=åˆ é™¤æ—§é›†åˆé‡æ–°å¯¼, False=è¿½åŠ æ•°æ®)
RESET_COLLECTION = True

# 5. åµŒå…¥ç»´åº¦ (è·Ÿæ¨¡å‹ä¿æŒä¸€è‡´)
EMBEDDING_DIM = getattr(config, "EMBEDDING_DIM", 4096)

# 6. å†™å…¥æ‰¹æ¬¡å¤§å° (æ¯å¤„ç†å¤šå°‘æ¡å†™ä¸€æ¬¡åº“ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º)
BATCH_SIZE = 30

# ==============================================================================


# === 1. é€‚é…å™¨å®šä¹‰ ===
class LocalEmbeddingAdapter(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        return call_ai_emb(input, dimensions=EMBEDDING_DIM)


# === 2. åˆå§‹åŒ–æ•°æ®åº“ ===
def init_vector_db():
    print(f"ğŸ”Œ è¿æ¥å‘é‡æ•°æ®åº“: {VECTOR_DB_PATH}")
    if not os.path.exists(VECTOR_DB_PATH):
        os.makedirs(VECTOR_DB_PATH)

    client = chromadb.PersistentClient(path=VECTOR_DB_PATH)

    # å¦‚æœéœ€è¦é‡ç½®ï¼Œå…ˆåˆ é™¤
    if RESET_COLLECTION:
        try:
            print(f"ğŸ—‘ï¸ æ­£åœ¨æ¸…ç©ºé›†åˆ [{TARGET_COLLECTION_NAME}] ...")
            client.delete_collection(TARGET_COLLECTION_NAME)
            print("âœ… æ—§é›†åˆå·²åˆ é™¤")
        except ValueError:
            print(f"â„¹ï¸ é›†åˆä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤é›†åˆæ—¶æŠ¥é”™ (å¯å¿½ç•¥): {e}")

    # åˆ›å»º/è·å–é›†åˆ
    collection = client.get_or_create_collection(
        name=TARGET_COLLECTION_NAME,
        embedding_function=LocalEmbeddingAdapter(),
        metadata={"description": "å•æ–‡ä»¶å¯¼å…¥"}
    )
    return client, collection


# === 3. æ ¸å¿ƒå…¥åº“é€»è¾‘ ===
def import_specific_json():
    # 0. æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(TARGET_JSON_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {TARGET_JSON_PATH}")
        return

    # 1. åˆå§‹åŒ– DB
    client, collection = init_vector_db()

    # 2. è¯»å– JSON
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {os.path.basename(TARGET_JSON_PATH)}")
    try:
        with open(TARGET_JSON_PATH, "r", encoding="utf-8") as f:
            fragments = json.load(f)
    except Exception as e:
        print(f"âŒ JSON æ ¼å¼é”™è¯¯æˆ–æ— æ³•è¯»å–: {e}")
        return

    # 3. è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_frags = [f for f in fragments if f.get("ç‰‡æ®µå†…å®¹") and len(f.get("ç‰‡æ®µå†…å®¹").strip()) > 1]
    total_count = len(valid_frags)
    print(f"ğŸ“Š æœ‰æ•ˆç‰‡æ®µæ•°: {total_count}")

    if total_count == 0:
        print("âš ï¸ æ•°æ®ä¸ºç©ºï¼Œæ— éœ€å¯¼å…¥")
        return

    # 4. éå†å¹¶æ‰¹é‡å†™å…¥
    batch_data = {"ids": [], "documents": [], "metadatas": [], "embeddings": []}
    imported_count = 0
    file_prefix = os.path.splitext(os.path.basename(TARGET_JSON_PATH))[0]

    for idx, frag in enumerate(valid_frags):
        # A. ç¡®å®šå‘é‡æ–‡æœ¬ (ä¼˜å…ˆç”¨é¢„å¤„ç†å¥½çš„ï¼Œæ²¡æœ‰åˆ™æ‰‹åŠ¨æ‹¼æ¥)
        vec_text = frag.get("å‘é‡æ–‡æœ¬")
        if not vec_text:
            vec_text = f"{frag.get('ç»„åˆæ ‡é¢˜', '')}ï¼š\n{frag.get('ç‰‡æ®µå†…å®¹', '')}"

        # B. å‘é‡åŒ–
        emb = call_ai_emb(vec_text, dimensions=EMBEDDING_DIM)
        if not emb:
            print(f"âš ï¸ ç¬¬ {idx} æ¡å‘é‡åŒ–å¤±è´¥ï¼Œè·³è¿‡")
            continue

        # C. æ„å»º Metadata (è½¬ä¸ºå­—ç¬¦ä¸²ä»¥é˜²æŠ¥é”™)
        meta = {
            "æ¥æºæ–‡ä»¶": str(frag.get("æ¥æºæ–‡ä»¶", file_prefix)),
            "å®Œæ•´è·¯å¾„": str(frag.get("å®Œæ•´è·¯å¾„", "")),
            "ç»„åˆæ ‡é¢˜": str(frag.get("ç»„åˆæ ‡é¢˜", "")),
            "å­—æ•°": int(frag.get("å­—æ•°", len(vec_text))),
            "ç‰‡æ®µå†…å®¹": str(frag.get("ç‰‡æ®µå†…å®¹", ""))[:3000] # é˜²æ­¢è¶…é•¿
        }
        for i in range(1, 9):
            meta[f"L{i}"] = str(frag.get(f"L{i}", ""))

        # D. æ”¾å…¥æ‰¹æ¬¡
        unique_id = f"{file_prefix}_{idx}"
        batch_data["ids"].append(unique_id)
        batch_data["documents"].append(vec_text)
        batch_data["metadatas"].append(meta)
        batch_data["embeddings"].append(emb)

        # E. æ‰¹æ¬¡å†™å…¥
        if len(batch_data["ids"]) >= BATCH_SIZE:
            collection.add(
                ids=batch_data["ids"],
                documents=batch_data["documents"],
                metadatas=batch_data["metadatas"],
                embeddings=batch_data["embeddings"]
            )
            imported_count += len(batch_data["ids"])
            print(f"   â³ å·²å¯¼å…¥ {imported_count}/{total_count} ...")
            for k in batch_data: batch_data[k] = [] # æ¸…ç©º

    # 5. å¤„ç†å‰©ä½™æ•°æ®
    if batch_data["ids"]:
        collection.add(
            ids=batch_data["ids"],
            documents=batch_data["documents"],
            metadatas=batch_data["metadatas"],
            embeddings=batch_data["embeddings"]
        )
        imported_count += len(batch_data["ids"])

    print("\n" + "="*50)
    print(f"ğŸ‰ å…¥åº“å®Œæˆï¼")
    print(f"ğŸ“‚ æ–‡ä»¶: {os.path.basename(TARGET_JSON_PATH)}")
    print(f"ğŸ—„ï¸ é›†åˆ: {TARGET_COLLECTION_NAME}")
    print(f"ğŸ“ˆ æˆåŠŸå¯¼å…¥: {imported_count} æ¡")
    print("="*50)

if __name__ == "__main__":
    import_specific_json()