import os
import sys
import chromadb
import time
from collections import defaultdict

# === 1. ç¯å¢ƒè·¯å¾„ä¿®å¤ ===
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

# å¯¼å…¥é…ç½®å’Œå·¥å…·
from config import config
from backend.tools.tools_sql_connect import db
from backend.tools.tools_call_ai import call_ai_emb

# === 2. é…ç½® ===
VECTOR_DB_PATH = getattr(config, "VECTOR_DB_PATH_MEDIC", "G:/KnowledgeBase/vectorizer_medic")
COLLECTION_NAME = "Case_Question"
EMBEDDING_DIM = getattr(config, "EMBEDDING_DIM", 4096)


# ==================== æ ¸å¿ƒé€»è¾‘ ====================

def reset_collection():
    """å¼ºåˆ¶åˆ é™¤å¹¶é‡æ–°åˆ›å»ºé›†åˆ"""
    print(f"ğŸ§¹ æ­£åœ¨æ¸…ç†å‘é‡åº“é›†åˆ: {COLLECTION_NAME} ...")
    if not os.path.exists(VECTOR_DB_PATH):
        os.makedirs(VECTOR_DB_PATH, exist_ok=True)

    client = chromadb.PersistentClient(path=VECTOR_DB_PATH)

    try:
        client.delete_collection(COLLECTION_NAME)
        print("   - æ—§é›†åˆå·²åˆ é™¤")
    except:
        pass  # é›†åˆä¸å­˜åœ¨åˆ™å¿½ç•¥

    # é‡å»º
    collection = client.create_collection(
        name=COLLECTION_NAME,
        metadata={"description": "æ¡ˆä¾‹åˆ†æé¢˜åº“ï¼ˆèšåˆç‰ˆï¼‰ï¼šä¸€ä¸ªVectorå¯¹åº”ä¸€ä¸ªæ¡ˆä¾‹+å¤šä¸ªé—®é¢˜"}
    )
    print("   - æ–°é›†åˆåˆ›å»ºæˆåŠŸ")
    return client, collection


def fetch_and_group_data():
    """ä» MySQL è·å–å¹¶æŒ‰æ¡ˆä¾‹åˆ†ç»„"""
    print("ğŸ“¡ æ­£åœ¨ä» MySQL è¯»å–æ•°æ®...")
    sql = "SELECT * FROM case_question ORDER BY question_id ASC"  # æ’åºä¿è¯é¡ºåº
    rows = db.execute_query(sql)
    print(f"   - åŸå§‹é¢˜ç›®æ•°é‡: {len(rows)}")

    grouped_data = []

    # ä¸´æ—¶å­—å…¸ç”¨äºåˆ†ç»„ï¼š key=case_content_hash, value=group_obj
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ case_content ç›¸åŒçš„å³ä¸ºåŒä¸€ç»„
    groups_map = defaultdict(lambda: {
        "case_content": "",
        "questions": [],
        "ids": [],
        "sources": [],
        "answers": []
    })

    # ç‹¬ç«‹é¢˜ç›®åˆ—è¡¨ï¼ˆæ²¡æœ‰å…±ç”¨é¢˜å¹²çš„ï¼‰
    standalone_items = []

    for row in rows:
        case_txt = row.get('case_content', '')

        if case_txt and len(case_txt.strip()) > 5:
            # æœ‰æ¡ˆä¾‹èƒŒæ™¯ï¼Œå½’å…¥ç»„
            # ä½¿ç”¨å†…å®¹ä½œä¸º Key (å»é™¤é¦–å°¾ç©ºæ ¼)
            key = case_txt.strip()
            groups_map[key]["case_content"] = key
            groups_map[key]["questions"].append(row['stem'])
            groups_map[key]["ids"].append(str(row['question_id']))
            groups_map[key]["sources"].append(str(row.get('source', '')))
            groups_map[key]["answers"].append(row['answer'])
        else:
            # æ— æ¡ˆä¾‹èƒŒæ™¯ï¼Œä½œä¸ºå•é¢˜å¤„ç†
            standalone_items.append(row)

    # å°† Map è½¬ä¸º List
    grouped_data = list(groups_map.values())

    print(f"âœ… åˆ†ç»„å®Œæˆï¼š")
    print(f"   - å…±ç”¨æ¡ˆä¾‹ç»„: {len(grouped_data)} ç»„ (åŒ…å«å¤šä¸ªå°é¢˜)")
    print(f"   - ç‹¬ç«‹å°é¢˜: {len(standalone_items)} æ¡")

    return grouped_data, standalone_items


def process_import():
    client, collection = reset_collection()
    grouped_data, standalone_items = fetch_and_group_data()

    total_tasks = len(grouped_data) + len(standalone_items)
    print(f"ğŸš€ å¼€å§‹å‘é‡åŒ–å¹¶å…¥åº“ï¼Œå…± {total_tasks} ä¸ªå‘é‡æ¡ç›®...")

    count = 0

    # --- 1. å¤„ç†å…±ç”¨æ¡ˆä¾‹ç»„ ---
    for group in grouped_data:
        try:
            # æ„é€ èšåˆæ–‡æœ¬
            # æ ¼å¼ï¼š
            # ã€å…±ç”¨æ¡ˆä¾‹ã€‘...
            # ã€é—®é¢˜1ã€‘... (ç­”æ¡ˆ: A)
            # ã€é—®é¢˜2ã€‘... (ç­”æ¡ˆ: B)

            combined_text = f"ã€å…±ç”¨æ¡ˆä¾‹ã€‘\n{group['case_content']}\n"
            for i, stem in enumerate(group['questions']):
                ans = group['answers'][i]
                combined_text += f"\nã€é—®é¢˜{i + 1}ã€‘{stem}\n(ç­”æ¡ˆ: {ans})"

            # æ„é€  Metadata
            # db_ids å­˜ä¸º "101,102,103"
            meta = {
                "db_ids": ",".join(group['ids']),
                "source_ids": ",".join(group['sources']),
                "type": "grouped_case",
                "question_count": len(group['ids']),
                "preview": group['case_content'][:50]  # é¢„è§ˆç”¨
            }

            # ID ä½¿ç”¨ç¬¬ä¸€ä¸ªé¢˜ç›®çš„ source_id åŠ åç¼€
            unique_id = f"group_{group['sources'][0]}"

            # å‘é‡åŒ–
            emb = call_ai_emb(combined_text)
            if emb:
                collection.add(
                    ids=[unique_id],
                    documents=[combined_text],
                    embeddings=[emb],
                    metadatas=[meta]
                )
                count += 1
                print(f"   [Group] å­˜å…¥ç»„ ID: {unique_id} (å« {len(group['ids'])} é¢˜)")

        except Exception as e:
            print(f"   âŒ å¤„ç†ç»„å¤±è´¥: {e}")

    # --- 2. å¤„ç†ç‹¬ç«‹é¢˜ç›® ---
    for row in standalone_items:
        try:
            vector_text = f"ã€é—®é¢˜ã€‘{row['stem']}\n(ç­”æ¡ˆ: {row['answer']})"

            meta = {
                "db_ids": str(row['question_id']),
                "source_ids": str(row.get('source', '')),
                "type": "single_question",
                "question_count": 1,
                "preview": row['stem'][:50]
            }

            unique_id = f"single_{row.get('source', row['question_id'])}"

            emb = call_ai_emb(vector_text)
            if emb:
                collection.add(
                    ids=[unique_id],
                    documents=[vector_text],
                    embeddings=[emb],
                    metadatas=[meta]
                )
                count += 1
        except Exception as e:
            print(f"   âŒ å¤„ç†å•é¢˜å¤±è´¥: {e}")

    print("=" * 50)
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"   - å®é™…å­˜å…¥å‘é‡åº“æ¡ç›®: {count}")
    print(f"   - å‘é‡åº“å½“å‰æ€»æ•°: {collection.count()}")


if __name__ == "__main__":
    process_import()